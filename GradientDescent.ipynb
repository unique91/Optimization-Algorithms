{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpJUTnLK0lGY2aHlcbYs/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unique91/Optimization-Algorithms/blob/main/GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1GwkbAr4SUoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "block_plot = False"
      ],
      "metadata": {
        "id": "EoB2fd-7TzPN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data():\n",
        "  # Random manual seed for consistency\n",
        "  tf.random.set_seed(42)\n",
        "  num_data = 30\n",
        "  # Create some data that is rougly linear\n",
        "  x = 10 * tf.random.uniform(shape=[num_data])\n",
        "  y = x + tf.random.normal(stddev=0.3, shape=[num_data])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "x, y = create_data()"
      ],
      "metadata": {
        "id": "R2228KATWCTw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter settings\n",
        "num_iter0 = 50\n",
        "lr0 = 0.005\n",
        "# Initial guess for m\n",
        "m0 = 2\n",
        "max_los = 30. # For plot scale"
      ],
      "metadata": {
        "id": "TqpNBJBmT17u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_linear_model(x, y, m_best, xlim=(0, 10), ylim=(0, 10)):\n",
        "  # Generate the line based on the optimal slope.\n",
        "  xmin = tf.reduce_min(x)\n",
        "  xmax = tf.reduce_max(x)\n",
        "  ymin = tf.reduce_min(y)\n",
        "  ymax = tf.reduce_max(y)\n",
        "\n",
        "  xplot = np.linspace(xmin, xmax, 2)\n",
        "  yplot = m_best * xplot\n",
        "\n",
        "  # Plot the data and the model.\n",
        "  plt.figure\n",
        "  plt.xlim(xlim); plt.ylim(ylim)\n",
        "  plt.plot(xplot, yplot, 'c-')\n",
        "  plt.scatter(x, y, color='blue', s=20)\n",
        "  plt.xlabel('x'); plt.ylabel('y')\n",
        "  xc = .05 * (xmax - xmin)\n",
        "  yc = .95 * (ymax - ymin)\n",
        "  plt.text(xc, yc, 'Slope: ' + str(int(m_best*1000)/1000), fontsize=14)\n",
        "  plt.show(block=block_plot)"
      ],
      "metadata": {
        "id": "AiKkCbrLYxHT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iter = num_iter0\n",
        "lr = lr0\n",
        "m = m0\n",
        "\n",
        "# Loss for gradient descent\n",
        "loss_gd = tf.Variable(tf.zeros(shape=[num_iter]))\n",
        "\n",
        "# Calculate the loss\n",
        "for i in range(0, num_iter):\n",
        "  g = -2 * tf.reduce_sum(x * (y - m * x)) / len(x)\n",
        "  m = m - lr * g\n",
        "  # Compute the loss for the update value of m\n",
        "  e = y - m * x\n",
        "  loss_gd[i].assign(tf.reduce_sum(tf.multiply(e,e))/len(x))\n",
        "\n",
        "m_best = m.numpy()\n",
        "print('Minimum loss: ', loss_gd[-1].numpy())\n",
        "print('Best parameter: ', m_best)\n",
        "\n",
        "# Plot loss vs m\n",
        "plt.figure\n",
        "plt.plot(loss_gd.numpy(), 'c-')\n",
        "plt.xlim(0, num_iter); plt.ylim(0, max_los)\n",
        "plt.ylabel('Loss'); plt.xlabel('Iterations')\n",
        "plt.title('Gradient Descent')\n",
        "plt.show(block=block_plot)\n",
        "\n",
        "plot_linear_model(x, y, m_best)"
      ],
      "metadata": {
        "id": "nmTbW24UVdZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iter = num_iter0\n",
        "lr = lr0\n",
        "m = m0\n",
        "\n",
        "# Loss for stochastic gradient descent\n",
        "loss_sgd = tf.Variable(tf.zeros(num_iter))\n",
        "\n",
        "for i in range(0, num_iter):\n",
        "  # Randomly select a training data point\n",
        "  k = tf.random.uniform([1], minval=0, maxval=len(y), dtype=tf.dtypes.int32)\n",
        "\n",
        "  # Calculate the gradient using a single data point\n",
        "  g = -2 * tf.gather(x, k) * (tf.gather(y, k) - m * tf.gather(x, k))\n",
        "\n",
        "  # Update the parameter m\n",
        "  m = m - lr * g\n",
        "\n",
        "  # Compute the loss for the updated value of m\n",
        "  e = y - m * x\n",
        "  loss_sgd[i].assign(tf.reduce_sum(tf.multiply(e, e)))\n",
        "\n",
        "m_best = m.numpy()\n",
        "print('Minimum loss: ', loss_sgd[-1].numpy())\n",
        "print('Best parameter: ', m_best)\n",
        "\n",
        "# Plot loss vs m\n",
        "plt.figure\n",
        "plt.plot(loss_sgd.numpy(), 'c-')\n",
        "plt.xlim(0, num_iter); plt.ylim(0, max_los)\n",
        "plt.ylabel('Loss'); plt.xlabel('Iterations')\n",
        "plt.title('Stochastic Gradient Descent')\n",
        "plt.show(block=block_plot)\n",
        "\n",
        "plot_linear_model(x, y, m_best)"
      ],
      "metadata": {
        "id": "d7BnYrSrkTX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iter = num_iter0\n",
        "lr = lr0\n",
        "m = m0\n",
        "batch_size = 10\n",
        "\n",
        "# Loss for Gradient Descent with Mini-Batch\n",
        "loss_sgd_mb = tf.Variable(tf.zeros(num_iter))\n",
        "\n",
        "for i in range(0, num_iter):\n",
        "  # Randomly select a batch of data points\n",
        "  k = tf.random.uniform([batch_size], minval=0, maxval=len(y)-1, dtype=tf.dtypes.int32)\n",
        "\n",
        "  # Calculate the gradient using a mini-batch\n",
        "  g = -2 * tf.reduce_sum(tf.gather(x, k) * (tf.gather(y, k) - m * tf.gather(x, k))) / batch_size\n",
        "\n",
        "  # Update the parameter m\n",
        "  m = m - lr * g\n",
        "\n",
        "  # Compute the loss for the update value of m\n",
        "  e = y - m * x\n",
        "  loss_sgd_mb[i].assign(tf.reduce_sum(tf.multiply(e, e)) / batch_size)\n",
        "\n",
        "b_best = m.numpy()\n",
        "\n",
        "print('Minimum loss: ', loss_sgd_mb[-1].numpy())\n",
        "print('Best parameter: ', m.numpy())\n",
        "\n",
        "# Plot loss vs m\n",
        "plt.figure\n",
        "plt.plot(loss_sgd_mb.numpy(), 'c-')\n",
        "plt.xlim(0, num_iter); plt.ylim(0, max_los)\n",
        "plt.ylabel('Loss'); plt.xlabel('Iterations')\n",
        "plt.title('Stochastic Gradient Descent with Mini-Batch')\n",
        "plt.show(block=block_plot)\n",
        "\n",
        "plot_linear_model(x, y, b_best)"
      ],
      "metadata": {
        "id": "Yx9DBtUsrptf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare all three methods together\n",
        "plt.figure(figsize=(20, 8))\n",
        "\n",
        "plt.subplot(131); plt.plot(loss_gd.numpy(), 'c-'); plt.xlim(0, num_iter); plt.ylim(0, max_los);\n",
        "plt.ylabel('loss'); plt.xlabel('iterations'); plt.title('Gradient Descent')\n",
        "\n",
        "plt.subplot(132); plt.plot(loss_sgd.numpy(), 'c-'); plt.xlim(0, num_iter); plt.ylim(0, max_los);\n",
        "plt.ylabel('loss'); plt.xlabel('iterations'); plt.title('Stochastic Gradient Descent')\n",
        "\n",
        "plt.subplot(133); plt.plot(loss_sgd_mb.numpy(), 'c-'); plt.xlim(0, num_iter); plt.ylim(0, max_los);\n",
        "plt.ylabel('loss'); plt.xlabel('iterations'); plt.title('Stochastic Gradient Descent with Mini-Batch')"
      ],
      "metadata": {
        "id": "XsRym8clOdeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}